{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24630b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnsemble Learning - Detailed Explanation with Python Comments\\n\\nEnsemble Learning is a machine learning paradigm where multiple models \\n(often called \"weak learners\" or \"base learners\") are combined \\nto produce a single predictive model. The main idea is to improve \\nperformance and robustness over any individual model.\\n\\nWhy use Ensemble Learning?\\n- Individual models have limitations: prone to overfitting, bias, variance, etc.\\n- Combining models reduces errors caused by bias, variance, or noise.\\n- Ensembles generally achieve better accuracy, stability, and generalization.\\n\\nTypes of Ensemble Learning:\\n1. Bagging (Bootstrap Aggregating)\\n   - Train multiple models independently on different random subsets of data.\\n   - Combine their predictions by voting (classification) or averaging (regression).\\n   - Example: Random Forests.\\n\\n2. Boosting\\n   - Train models sequentially, where each new model focuses on the mistakes of the previous ones.\\n   - Combine models by weighted voting or summing predictions.\\n   - Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost.\\n\\n3. Stacking (Stacked Generalization)\\n   - Train multiple base models on the same dataset.\\n   - Train a meta-model to combine base model outputs for final prediction.\\n\\n4. Voting\\n   - Combine predictions from multiple different models by majority vote (classification) or averaging (regression).\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble Learning - Detailed Explanation with Python Comments\n",
    "\n",
    "Ensemble Learning is a machine learning paradigm where multiple models \n",
    "(often called \"weak learners\" or \"base learners\") are combined \n",
    "to produce a single predictive model. The main idea is to improve \n",
    "performance and robustness over any individual model.\n",
    "\n",
    "Why use Ensemble Learning?\n",
    "- Individual models have limitations: prone to overfitting, bias, variance, etc.\n",
    "- Combining models reduces errors caused by bias, variance, or noise.\n",
    "- Ensembles generally achieve better accuracy, stability, and generalization.\n",
    "\n",
    "Types of Ensemble Learning:\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "   - Train multiple models independently on different random subsets of data.\n",
    "   - Combine their predictions by voting (classification) or averaging (regression).\n",
    "   - Example: Random Forests.\n",
    "\n",
    "2. Boosting\n",
    "   - Train models sequentially, where each new model focuses on the mistakes of the previous ones.\n",
    "   - Combine models by weighted voting or summing predictions.\n",
    "   - Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost.\n",
    "\n",
    "3. Stacking (Stacked Generalization)\n",
    "   - Train multiple base models on the same dataset.\n",
    "   - Train a meta-model to combine base model outputs for final prediction.\n",
    "\n",
    "4. Voting\n",
    "   - Combine predictions from multiple different models by majority vote (classification) or averaging (regression).\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0eaf20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Decision Tree Accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingle Decision Tree Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_test,\u001b[38;5;250m \u001b[39my_pred_single)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Initialize Bagging Classifier with decision trees as base learners\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m bagging \u001b[38;5;241m=\u001b[39m \u001b[43mBaggingClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDecisionTreeClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Number of trees in ensemble\u001b[39;49;00m\n\u001b[0;32m     86\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Each sample size = original dataset size\u001b[39;49;00m\n\u001b[0;32m     87\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Use bootstrap sampling\u001b[39;49;00m\n\u001b[0;32m     88\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# Use all CPU cores for parallelism\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Train Bagging ensemble\u001b[39;00m\n\u001b[0;32m     92\u001b[0m bagging\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bagging (Bootstrap Aggregating) - Detailed Explanation\n",
    "\n",
    "Definition:\n",
    "Bagging is an ensemble learning technique designed to improve the stability \n",
    "and accuracy of machine learning algorithms by reducing variance and helping \n",
    "to avoid overfitting. It works by creating multiple versions of a predictor \n",
    "and using these to get an aggregated prediction.\n",
    "\n",
    "How Bagging Works (Step-by-step):\n",
    "\n",
    "1. Bootstrap Sampling:\n",
    "   - From the original training dataset with N samples, generate multiple \n",
    "     new training datasets (called bootstrap samples).\n",
    "   - Each bootstrap sample is created by randomly selecting N samples from \n",
    "     the original dataset **with replacement**.\n",
    "   - Because of replacement, some original samples appear multiple times, \n",
    "     while others may be missing in each bootstrap sample.\n",
    "\n",
    "2. Train Base Learners:\n",
    "   - For each bootstrap sample, train a base learner (often a weak model like \n",
    "     a decision tree).\n",
    "   - Each base learner is trained independently on its own bootstrap sample.\n",
    "\n",
    "3. Aggregate Predictions:\n",
    "   - For classification: aggregate by majority voting (the class predicted by \n",
    "     most models becomes the final prediction).\n",
    "   - For regression: aggregate by averaging predictions from all models.\n",
    "\n",
    "Why Bagging Works:\n",
    "- By training on different samples, each base learner sees a slightly different \n",
    "  dataset and thus makes different errors.\n",
    "- Aggregating predictions reduces the overall variance and smooths out \n",
    "  overfitting from individual models.\n",
    "- The ensemble typically outperforms any single base learner.\n",
    "\n",
    "Important Characteristics:\n",
    "- Parallelizable: Each base learner can be trained independently.\n",
    "- Reduces variance but does NOT reduce bias significantly.\n",
    "- Works best with unstable base learners (models that are sensitive to training \n",
    "  data variations), e.g., decision trees.\n",
    "\n",
    "Example use case:\n",
    "- Random Forest is a popular bagging method that builds many decision trees on \n",
    "  bootstrap samples and introduces additional randomness in feature selection.\n",
    "\n",
    "Mathematical Intuition:\n",
    "- Suppose the base learner has error variance σ².\n",
    "- If M independent models are averaged, variance reduces roughly to σ²/M.\n",
    "- Hence, ensemble reduces variance and improves robustness.\n",
    "\n",
    "Limitations:\n",
    "- Bagging does not improve bias (systematic errors).\n",
    "- If base learner is very stable (like linear regression), bagging provides \n",
    "  little benefit.\n",
    "\n",
    "--------------------------------------------------------\n",
    "Simple Python example to illustrate Bagging with Decision Trees\n",
    "--------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a single Decision Tree (base learner)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train and test single decision tree for baseline\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_single = dt.predict(X_test)\n",
    "print(f\"Single Decision Tree Accuracy: {accuracy_score(y_test, y_pred_single):.4f}\")\n",
    "\n",
    "# Initialize Bagging Classifier with decision trees as base learners\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                            n_estimators=100,        # Number of trees in ensemble\n",
    "                            max_samples=1.0,         # Each sample size = original dataset size\n",
    "                            bootstrap=True,          # Use bootstrap sampling\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1)               # Use all CPU cores for parallelism\n",
    "\n",
    "# Train Bagging ensemble\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f\"Bagging Ensemble Accuracy: {accuracy_score(y_test, y_pred_bagging):.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Detailed Breakdown:\n",
    "\n",
    "- base_estimator=DecisionTreeClassifier(): Decision tree is unstable, making it ideal for bagging.\n",
    "- n_estimators=100: Number of base learners trained on different bootstrap samples.\n",
    "- max_samples=1.0: Each bootstrap sample is same size as original dataset.\n",
    "- bootstrap=True: Sampling with replacement (bootstrap).\n",
    "- n_jobs=-1: Use all CPU cores to speed up training in parallel.\n",
    "\n",
    "Observations:\n",
    "- Accuracy of the Bagging ensemble is usually better than single decision tree.\n",
    "- Bagging reduces variance, leading to more stable and accurate predictions.\n",
    "- Since models are independent, parallel training speeds up computation.\n",
    "\n",
    "Summary:\n",
    "Bagging is an effective and simple ensemble technique mainly focused on reducing variance.\n",
    "It's ideal for models prone to overfitting on small data variations.\n",
    "Random Forest is a special case of bagging that adds feature randomness.\n",
    "\"\"\"\n",
    "\n",
    "# End of explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Boosting - Detailed Explanation\n",
    "\n",
    "Definition:\n",
    "Boosting is an ensemble learning technique that aims to create a strong classifier \n",
    "by sequentially combining multiple weak classifiers. Unlike bagging, where base \n",
    "learners are trained independently, boosting trains models sequentially, each \n",
    "trying to correct the errors of the previous ones.\n",
    "\n",
    "Core Idea:\n",
    "- Start with a weak learner (slightly better than random guessing).\n",
    "- After each model is trained, increase the weight (importance) of the samples \n",
    "  that were misclassified.\n",
    "- The next learner focuses more on these \"hard\" samples.\n",
    "- Final prediction is a weighted combination of all base learners' predictions.\n",
    "\n",
    "Why Boosting Works:\n",
    "- By focusing on mistakes, the ensemble progressively reduces bias.\n",
    "- Combines weak models into a strong one.\n",
    "- Often achieves higher accuracy than bagging but may be more prone to overfitting if not regularized.\n",
    "\n",
    "Key Characteristics:\n",
    "- Models are trained sequentially, not independently.\n",
    "- Each subsequent model focuses on the errors of the prior ensemble.\n",
    "- Outputs a weighted vote or sum of predictions.\n",
    "- Can reduce both bias and variance.\n",
    "- Usually requires careful tuning (number of learners, learning rate).\n",
    "\n",
    "Popular Boosting Algorithms:\n",
    "1. AdaBoost (Adaptive Boosting)\n",
    "2. Gradient Boosting Machines (GBM)\n",
    "3. XGBoost (Extreme Gradient Boosting)\n",
    "4. LightGBM, CatBoost (optimized gradient boosting variants)\n",
    "\n",
    "Mathematical Intuition (AdaBoost):\n",
    "- Assign equal weights to all training samples initially.\n",
    "- Train weak learner on weighted data.\n",
    "- Calculate weighted error rate.\n",
    "- Compute learner's weight based on error (more accurate learner gets higher weight).\n",
    "- Increase weights of misclassified samples so next learner focuses on them.\n",
    "- Final prediction: weighted majority vote of all learners.\n",
    "\n",
    "--------------------------------------------------------\n",
    "Python example: AdaBoost with Decision Trees\n",
    "--------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Weak learner: Decision Tree stump (depth=1)\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Initialize AdaBoost classifier with 50 weak learners\n",
    "ada = AdaBoostClassifier(base_estimator=weak_learner, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Train AdaBoost model\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = ada.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f\"AdaBoost Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Detailed Explanation:\n",
    "\n",
    "- base_estimator=DecisionTreeClassifier(max_depth=1): Weak learner with low complexity.\n",
    "- n_estimators=50: Number of boosting rounds (weak learners).\n",
    "- learning_rate=1.0: Controls contribution of each learner to final prediction (can be tuned to avoid overfitting).\n",
    "\n",
    "How AdaBoost Works Internally:\n",
    "\n",
    "1. Initialize sample weights equally.\n",
    "2. Train the first weak learner.\n",
    "3. Compute error weighted by sample weights.\n",
    "4. Compute learner weight = log((1 - error) / error).\n",
    "5. Increase weights of misclassified samples.\n",
    "6. Normalize weights so they sum to 1.\n",
    "7. Train next learner on updated weights.\n",
    "8. Repeat steps 3-7 for all learners.\n",
    "9. Final prediction: weighted sum of all learners' predictions.\n",
    "\n",
    "Advantages of Boosting:\n",
    "- Improves model accuracy by focusing on difficult samples.\n",
    "- Can reduce both bias and variance.\n",
    "- Works well with weak base learners.\n",
    "- Often produces state-of-the-art results.\n",
    "\n",
    "Disadvantages:\n",
    "- More prone to overfitting if too many estimators or improper tuning.\n",
    "- Sequential training means longer training times and less parallelism.\n",
    "- Sensitive to noisy data and outliers.\n",
    "\n",
    "Summary:\n",
    "Boosting is a powerful ensemble method that builds a strong model by combining many weak learners in a sequential manner, focusing on correcting errors step-by-step. It is widely used in competitions and real-world applications for its accuracy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# End of explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stacking (Stacked Generalization) - Detailed Explanation\n",
    "\n",
    "Definition:\n",
    "Stacking is an ensemble learning technique that combines multiple different \n",
    "base models (called level-0 models) by training a higher-level model \n",
    "(called meta-model or level-1 model) to learn how to best combine their predictions.\n",
    "\n",
    "How Stacking Works (Step-by-step):\n",
    "\n",
    "1. Train multiple base learners (can be different types of models) \n",
    "   on the original training data.\n",
    "\n",
    "2. Generate predictions from each base learner on a validation dataset \n",
    "   (or via cross-validation).\n",
    "\n",
    "3. Use these predictions as input features to train a meta-model.\n",
    "\n",
    "4. The meta-model learns to combine base learners' predictions optimally \n",
    "   to improve overall accuracy.\n",
    "\n",
    "5. For final prediction on unseen data:\n",
    "   - Get predictions from base learners.\n",
    "   - Feed these predictions to the meta-model.\n",
    "   - Meta-model outputs the final prediction.\n",
    "\n",
    "Key Points:\n",
    "- Base learners can be heterogeneous (different algorithms) or homogeneous.\n",
    "- Meta-model is often a simple model like Logistic Regression or Linear Regression.\n",
    "- Helps reduce bias and variance by leveraging complementary strengths of models.\n",
    "- More complex than bagging or boosting.\n",
    "- Requires careful validation to avoid overfitting (usually via cross-validation).\n",
    "\n",
    "Advantages:\n",
    "- Can combine diverse models to improve predictive performance.\n",
    "- Often outperforms individual models and simpler ensembles.\n",
    "- Flexible framework, adaptable to many problems.\n",
    "\n",
    "Disadvantages:\n",
    "- More complex to implement and tune.\n",
    "- Requires extra data handling to generate meta-features.\n",
    "- Training time is longer due to multiple models.\n",
    "- Risk of overfitting meta-model if validation is not done properly.\n",
    "\n",
    "--------------------------------------------------------\n",
    "Python example using sklearn's StackingClassifier\n",
    "--------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base learners (level-0 models)\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42))  # probability=True for meta-model to use probabilities\n",
    "]\n",
    "\n",
    "# Define meta learner (level-1 model)\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "# Initialize stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    passthrough=False,    # If True, original features are also passed to meta-model\n",
    "    cv=5                  # Use 5-fold CV to generate meta-features to avoid overfitting\n",
    ")\n",
    "\n",
    "# Train stacking model\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Detailed Explanation:\n",
    "\n",
    "- Base learners (Random Forest, Gradient Boosting, SVM) are trained on the training data.\n",
    "- The stacking classifier internally performs cross-validation on the training set to create meta-features:\n",
    "  predictions of base learners on held-out folds.\n",
    "- Meta learner (Logistic Regression) trains on these meta-features to learn how to combine base models.\n",
    "- Final predictions combine base learners' outputs through the meta learner.\n",
    "- passthrough=False means only base learners' predictions are input to meta-model, not original features.\n",
    "\n",
    "Why Use Stacking?\n",
    "- Different models capture different patterns and have different strengths.\n",
    "- Meta learner learns which models to trust more depending on the input.\n",
    "- Can boost overall predictive performance beyond bagging and boosting.\n",
    "\n",
    "Best Practices:\n",
    "- Use cross-validation to create meta-features to prevent overfitting.\n",
    "- Keep meta learner simple to avoid complexity.\n",
    "- Tune base learners individually before stacking.\n",
    "- Consider using probabilities (predict_proba) instead of hard predictions for richer meta-features.\n",
    "\n",
    "Summary:\n",
    "Stacking is a powerful ensemble technique that \"learns to learn\" by training a meta-model to combine base learners’ predictions, often achieving superior results by exploiting the diversity of models.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# End of explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1743d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Voting Ensemble - Detailed Explanation\n",
    "\n",
    "Definition:\n",
    "Voting is one of the simplest ensemble learning methods where multiple different \n",
    "models (called base learners) are combined by aggregating their predictions \n",
    "to form a final prediction.\n",
    "\n",
    "Types of Voting:\n",
    "1. Hard Voting (Majority Voting):\n",
    "   - Each base learner predicts a class label.\n",
    "   - The final class prediction is the one that gets the majority of votes \n",
    "     from the base learners.\n",
    "\n",
    "2. Soft Voting (Weighted Probability Averaging):\n",
    "   - Each base learner predicts class probabilities.\n",
    "   - The predicted probabilities are averaged (or weighted averaged).\n",
    "   - The class with the highest average probability is chosen as final prediction.\n",
    "   - Usually performs better than hard voting because it uses more information.\n",
    "\n",
    "Why Voting Works:\n",
    "- Combines multiple diverse models to reduce the risk of individual model errors.\n",
    "- Aggregating multiple opinions usually leads to better generalization.\n",
    "- Voting ensembles often improve accuracy and robustness compared to single models.\n",
    "\n",
    "Characteristics:\n",
    "- Models can be heterogeneous (different algorithms) or homogeneous.\n",
    "- Voting does not explicitly try to reduce bias or variance like boosting or bagging but improves stability.\n",
    "- Easy to implement and computationally efficient compared to stacking or boosting.\n",
    "\n",
    "Limitations:\n",
    "- Voting ensembles treat each base model equally (unless weights are applied).\n",
    "- Hard voting ignores confidence (probability) information.\n",
    "- Does not learn how to combine models optimally (unlike stacking).\n",
    "\n",
    "--------------------------------------------------------\n",
    "Python example using sklearn VotingClassifier\n",
    "--------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)  # probability=True for soft voting\n",
    "\n",
    "# Initialize Voting Classifier with hard voting\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', dt_clf), ('svm', svm_clf)],\n",
    "    voting='hard'  # Use majority voting on predicted classes\n",
    ")\n",
    "\n",
    "# Train hard voting ensemble\n",
    "voting_hard.fit(X_train, y_train)\n",
    "\n",
    "# Predict with hard voting\n",
    "y_pred_hard = voting_hard.predict(X_test)\n",
    "print(f\"Hard Voting Accuracy: {accuracy_score(y_test, y_pred_hard):.4f}\")\n",
    "\n",
    "# Initialize Voting Classifier with soft voting\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', dt_clf), ('svm', svm_clf)],\n",
    "    voting='soft'  # Average predicted probabilities\n",
    ")\n",
    "\n",
    "# Train soft voting ensemble\n",
    "voting_soft.fit(X_train, y_train)\n",
    "\n",
    "# Predict with soft voting\n",
    "y_pred_soft = voting_soft.predict(X_test)\n",
    "print(f\"Soft Voting Accuracy: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "\n",
    "- Hard Voting:\n",
    "  * Each model predicts a class label.\n",
    "  * Final prediction is the class that gets the most votes.\n",
    "  * Easy to understand and implement.\n",
    "  \n",
    "- Soft Voting:\n",
    "  * Each model outputs class probabilities.\n",
    "  * Probabilities are averaged across models.\n",
    "  * The class with the highest average probability is selected.\n",
    "  * Usually gives better results because it considers model confidence.\n",
    "  \n",
    "- Models Used:\n",
    "  * Logistic Regression, Decision Tree, and SVM provide diversity.\n",
    "  * SVM needs probability=True to output probabilities for soft voting.\n",
    "  \n",
    "- VotingClassifier:\n",
    "  * 'estimators' is a list of (name, model) tuples.\n",
    "  * 'voting' param controls hard or soft voting.\n",
    "\n",
    "Summary:\n",
    "Voting ensembles aggregate predictions of multiple base models either by majority vote (hard) or averaging probabilities (soft) to improve robustness and accuracy with minimal complexity.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# End of explanation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
